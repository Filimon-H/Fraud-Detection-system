{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3: Feature Engineering for Fraud Detection\n",
    "\n",
    "## Objective\n",
    "Create meaningful features that help identify fraud patterns:\n",
    "1. **Time-based features**: hour_of_day, day_of_week, time_since_signup\n",
    "2. **Velocity features**: transaction frequency per user in time windows\n",
    "3. **Data transformations**: scaling and encoding for modeling\n",
    "4. **Handle class imbalance**: SMOTE/undersampling on training data\n",
    "\n",
    "## Why These Features Matter\n",
    "- **Time features**: Fraudsters often operate at unusual hours or create accounts just before fraud\n",
    "- **Velocity features**: Rapid-fire transactions from one user signal automated fraud\n",
    "- **Proper encoding/scaling**: Required for many ML algorithms to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project imports\n",
    "from src.features.time_features import add_time_features, add_time_period_features\n",
    "from src.features.velocity import add_velocity_features, add_user_transaction_count\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data with Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with country from previous notebook\n",
    "DATA_PATH = project_root / \"data\" / \"processed\" / \"fraud_with_country.parquet\"\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    print(f\"Loaded data with country: {df.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Please run notebook 02 first to create: {DATA_PATH}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current columns and types\n",
    "print(\"Current columns:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time features\n",
    "df = add_time_features(df)\n",
    "\n",
    "print(\"New time features added:\")\n",
    "time_cols = ['hour_of_day', 'day_of_week', 'is_weekend', 'time_since_signup']\n",
    "df[time_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time period feature (morning/afternoon/evening/night)\n",
    "df = add_time_period_features(df)\n",
    "\n",
    "print(\"Time period distribution:\")\n",
    "print(df['purchase_time_period'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time_since_signup statistics\n",
    "print(\"\\ntime_since_signup statistics (in seconds):\")\n",
    "print(df['time_since_signup'].describe())\n",
    "\n",
    "# Convert to more interpretable units\n",
    "df['time_since_signup_hours'] = df['time_since_signup'] / 3600\n",
    "df['time_since_signup_days'] = df['time_since_signup'] / 86400\n",
    "\n",
    "print(\"\\ntime_since_signup (in hours):\")\n",
    "print(df['time_since_signup_hours'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time features vs fraud\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Hour of day fraud rate\n",
    "fraud_by_hour = df.groupby('hour_of_day')['class'].mean() * 100\n",
    "axes[0, 0].bar(fraud_by_hour.index, fraud_by_hour.values, color='#e74c3c')\n",
    "axes[0, 0].axhline(y=df['class'].mean()*100, color='black', linestyle='--', label='Overall')\n",
    "axes[0, 0].set_xlabel('Hour of Day')\n",
    "axes[0, 0].set_ylabel('Fraud Rate (%)')\n",
    "axes[0, 0].set_title('Fraud Rate by Hour of Day')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Day of week fraud rate\n",
    "fraud_by_dow = df.groupby('day_of_week')['class'].mean() * 100\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0, 1].bar(days, fraud_by_dow.values, color='#9b59b6')\n",
    "axes[0, 1].axhline(y=df['class'].mean()*100, color='black', linestyle='--', label='Overall')\n",
    "axes[0, 1].set_xlabel('Day of Week')\n",
    "axes[0, 1].set_ylabel('Fraud Rate (%)')\n",
    "axes[0, 1].set_title('Fraud Rate by Day of Week')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Time since signup distribution by class\n",
    "df_sample = df.sample(min(10000, len(df)), random_state=42)\n",
    "axes[1, 0].boxplot(\n",
    "    [df_sample[df_sample['class']==0]['time_since_signup_hours'].dropna(),\n",
    "     df_sample[df_sample['class']==1]['time_since_signup_hours'].dropna()],\n",
    "    labels=['Non-Fraud', 'Fraud']\n",
    ")\n",
    "axes[1, 0].set_ylabel('Time Since Signup (hours)')\n",
    "axes[1, 0].set_title('Time Since Signup by Class')\n",
    "\n",
    "# Time period fraud rate\n",
    "fraud_by_period = df.groupby('purchase_time_period')['class'].mean() * 100\n",
    "period_order = ['morning', 'afternoon', 'evening', 'night']\n",
    "fraud_by_period = fraud_by_period.reindex(period_order)\n",
    "axes[1, 1].bar(fraud_by_period.index, fraud_by_period.values, color='#3498db')\n",
    "axes[1, 1].axhline(y=df['class'].mean()*100, color='black', linestyle='--', label='Overall')\n",
    "axes[1, 1].set_xlabel('Time Period')\n",
    "axes[1, 1].set_ylabel('Fraud Rate (%)')\n",
    "axes[1, 1].set_title('Fraud Rate by Time Period')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Time Features\n",
    "\n",
    "*TODO: After running, describe:*\n",
    "- Are there hours with higher fraud rates?\n",
    "- Does day of week affect fraud rate?\n",
    "- Is time_since_signup different for fraud vs non-fraud? (This is often a strong signal!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Velocity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user transaction velocity (1 hour and 24 hour windows)\n",
    "print(\"Computing velocity features (this may take a moment)...\")\n",
    "df = add_velocity_features(df, user_col='user_id', time_col='purchase_time', windows_hours=[1, 24])\n",
    "\n",
    "print(\"\\nVelocity features added:\")\n",
    "velocity_cols = ['tx_count_user_id_1h', 'tx_count_user_id_24h']\n",
    "df[velocity_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add total user transaction count\n",
    "df = add_user_transaction_count(df, user_col='user_id')\n",
    "\n",
    "print(\"User total transactions:\")\n",
    "print(df['user_total_transactions'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize velocity features vs fraud\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Transaction count in 1h by class\n",
    "df_sample = df.sample(min(10000, len(df)), random_state=42)\n",
    "\n",
    "axes[0].boxplot(\n",
    "    [df_sample[df_sample['class']==0]['tx_count_user_id_1h'],\n",
    "     df_sample[df_sample['class']==1]['tx_count_user_id_1h']],\n",
    "    labels=['Non-Fraud', 'Fraud']\n",
    ")\n",
    "axes[0].set_ylabel('Transactions in 1 Hour')\n",
    "axes[0].set_title('User Transaction Velocity (1h) by Class')\n",
    "\n",
    "# Transaction count in 24h by class\n",
    "axes[1].boxplot(\n",
    "    [df_sample[df_sample['class']==0]['tx_count_user_id_24h'],\n",
    "     df_sample[df_sample['class']==1]['tx_count_user_id_24h']],\n",
    "    labels=['Non-Fraud', 'Fraud']\n",
    ")\n",
    "axes[1].set_ylabel('Transactions in 24 Hours')\n",
    "axes[1].set_title('User Transaction Velocity (24h) by Class')\n",
    "\n",
    "# Total user transactions by class\n",
    "axes[2].boxplot(\n",
    "    [df_sample[df_sample['class']==0]['user_total_transactions'],\n",
    "     df_sample[df_sample['class']==1]['user_total_transactions']],\n",
    "    labels=['Non-Fraud', 'Fraud']\n",
    ")\n",
    "axes[2].set_ylabel('Total User Transactions')\n",
    "axes[2].set_title('Total User Transactions by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "print(\"Velocity Features by Class:\")\n",
    "print(df.groupby('class')[velocity_cols + ['user_total_transactions']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Velocity Features\n",
    "\n",
    "*TODO: After running, describe:*\n",
    "- Do fraudulent users have higher transaction velocity?\n",
    "- Is there a difference in total transactions per user?\n",
    "- What velocity thresholds might indicate fraud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review all columns\n",
    "print(\"All columns after feature engineering:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "NUMERIC_FEATURES = [\n",
    "    'purchase_value',\n",
    "    'age',\n",
    "    'hour_of_day',\n",
    "    'day_of_week',\n",
    "    'is_weekend',\n",
    "    'time_since_signup',\n",
    "    'tx_count_user_id_1h',\n",
    "    'tx_count_user_id_24h',\n",
    "    'user_total_transactions'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'source',\n",
    "    'browser',\n",
    "    'sex',\n",
    "    'country'\n",
    "]\n",
    "\n",
    "TARGET = 'class'\n",
    "\n",
    "# Columns to exclude from features (identifiers, raw timestamps)\n",
    "EXCLUDE_COLS = [\n",
    "    'user_id', 'device_id', 'ip_address',\n",
    "    'signup_time', 'purchase_time',\n",
    "    'time_since_signup_hours', 'time_since_signup_days',\n",
    "    'purchase_time_period',  # Will use hour_of_day instead\n",
    "    'class'\n",
    "]\n",
    "\n",
    "print(f\"Numeric features ({len(NUMERIC_FEATURES)}): {NUMERIC_FEATURES}\")\n",
    "print(f\"Categorical features ({len(CATEGORICAL_FEATURES)}): {CATEGORICAL_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in features\n",
    "feature_cols = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "if missing.any():\n",
    "    print(\"Missing values in features:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values in selected features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "# Fill numeric with median\n",
    "for col in NUMERIC_FEATURES:\n",
    "    if df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"Filled {col} missing with median: {median_val}\")\n",
    "\n",
    "# Fill categorical with 'Unknown'\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        print(f\"Filled {col} missing with 'Unknown'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[NUMERIC_FEATURES + CATEGORICAL_FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformation (Scaling & Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), NUMERIC_FEATURES),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Transformed training shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed test shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after transformation\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(CATEGORICAL_FEATURES)\n",
    "all_feature_names = NUMERIC_FEATURES + list(cat_feature_names)\n",
    "\n",
    "print(f\"Total features after encoding: {len(all_feature_names)}\")\n",
    "print(f\"\\nFirst 20 feature names:\")\n",
    "print(all_feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution BEFORE resampling\n",
    "print(\"Class distribution BEFORE SMOTE:\")\n",
    "print(f\"  Non-Fraud (0): {(y_train == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {(y_train == 1).sum():,}\")\n",
    "print(f\"  Ratio: 1:{(y_train == 0).sum() / (y_train == 1).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "print(\"\\nClass distribution AFTER SMOTE:\")\n",
    "print(f\"  Non-Fraud (0): {(y_train_resampled == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {(y_train_resampled == 1).sum():,}\")\n",
    "print(f\"  Ratio: 1:{(y_train_resampled == 0).sum() / (y_train_resampled == 1).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Before SMOTE\n",
    "before_counts = [sum(y_train == 0), sum(y_train == 1)]\n",
    "axes[0].bar(['Non-Fraud', 'Fraud'], before_counts, color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Before SMOTE')\n",
    "for i, v in enumerate(before_counts):\n",
    "    axes[0].text(i, v + 100, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "after_counts = [sum(y_train_resampled == 0), sum(y_train_resampled == 1)]\n",
    "axes[1].bar(['Non-Fraud', 'Fraud'], after_counts, color=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('After SMOTE')\n",
    "for i, v in enumerate(after_counts):\n",
    "    axes[1].text(i, v + 100, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Class Imbalance Handling\n",
    "\n",
    "*TODO: After running, describe:*\n",
    "- What was the original imbalance ratio?\n",
    "- How many synthetic samples were created by SMOTE?\n",
    "- Why is it important to apply SMOTE only on training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full feature-engineered dataset (before train/test split)\n",
    "output_path = project_root / \"data\" / \"processed\" / \"fraud_featured.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Feature-engineered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/test splits as numpy arrays for modeling\n",
    "import joblib\n",
    "\n",
    "# Create models directory if needed\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, models_dir / \"preprocessor.joblib\")\n",
    "\n",
    "# Save data splits\n",
    "np.save(models_dir / \"X_train_resampled.npy\", X_train_resampled)\n",
    "np.save(models_dir / \"y_train_resampled.npy\", y_train_resampled)\n",
    "np.save(models_dir / \"X_test.npy\", X_test_transformed)\n",
    "np.save(models_dir / \"y_test.npy\", y_test.values)\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(all_feature_names, models_dir / \"feature_names.joblib\")\n",
    "\n",
    "print(f\"Saved preprocessor and data splits to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary: Task 1 Complete\n",
    "\n",
    "*TODO: Fill in after completing the analysis*\n",
    "\n",
    "### Features Created\n",
    "1. **Time features**: hour_of_day, day_of_week, is_weekend, time_since_signup\n",
    "2. **Velocity features**: tx_count_user_id_1h, tx_count_user_id_24h, user_total_transactions\n",
    "3. **Geographic feature**: country (from IP mapping)\n",
    "\n",
    "### Data Transformations\n",
    "- Numeric features scaled with StandardScaler\n",
    "- Categorical features encoded with OneHotEncoder\n",
    "- Total features after encoding: [number]\n",
    "\n",
    "### Class Imbalance\n",
    "- Original ratio: [ratio]\n",
    "- Applied SMOTE to training data only\n",
    "- After SMOTE ratio: 1:1\n",
    "\n",
    "### Key Insights\n",
    "1. [Insight about time features]\n",
    "2. [Insight about velocity features]\n",
    "3. [Insight about geographic patterns]\n",
    "\n",
    "### Files Saved\n",
    "- `data/processed/fraud_featured.parquet`: Full feature-engineered dataset\n",
    "- `models/preprocessor.joblib`: Fitted preprocessor for inference\n",
    "- `models/X_train_resampled.npy`, `y_train_resampled.npy`: Resampled training data\n",
    "- `models/X_test.npy`, `y_test.npy`: Test data\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to Task 2: Model Building and Training\n",
    "- Train baseline (Logistic Regression) and ensemble models\n",
    "- Evaluate using AUC-PR, F1-Score, Confusion Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
