{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1: Model Building - E-commerce Fraud Detection\n",
    "\n",
    "## Objective\n",
    "Build, train, and evaluate classification models to detect fraudulent e-commerce transactions:\n",
    "1. Train a **baseline** interpretable model (Logistic Regression)\n",
    "2. Train an **ensemble** model (Random Forest)\n",
    "3. Compare models using metrics appropriate for imbalanced data\n",
    "4. Select the best model with justification\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **AUC-PR (Average Precision)**: Primary metric for imbalanced data\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **Precision / Recall**: Trade-off analysis\n",
    "- **Confusion Matrix**: Detailed breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Project imports\n",
    "from src.modeling.pipelines import build_fraud_pipeline, get_model_name\n",
    "from src.modeling.train import train_and_evaluate, cross_validate_model, save_model, compare_models\n",
    "from src.modeling.metrics import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve,\n",
    "    get_classification_report_df,\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feature-Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered dataset from Task 1\n",
    "DATA_PATH = project_root / \"data\" / \"processed\" / \"fraud_featured.parquet\"\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    print(f\"Loaded feature-engineered data: {df.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Please run Task 1 notebooks first to create: {DATA_PATH}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "NUMERIC_FEATURES = [\n",
    "    'purchase_value', 'age', 'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "    'time_since_signup', 'tx_count_user_id_1h', 'tx_count_user_id_24h',\n",
    "    'user_total_transactions'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = ['source', 'browser', 'sex', 'country']\n",
    "\n",
    "TARGET = 'class'\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[NUMERIC_FEATURES + CATEGORICAL_FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "print(f\"Features: {X.shape[1]} columns\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logistic regression pipeline with SMOTE\n",
    "lr_pipeline = build_fraud_pipeline(\n",
    "    model_type=\"logistic\",\n",
    "    use_smote=True,\n",
    "    numeric_features=NUMERIC_FEATURES,\n",
    "    categorical_features=CATEGORICAL_FEATURES,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"Logistic Regression Pipeline:\")\n",
    "print(lr_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model, lr_metrics, lr_threshold = train_and_evaluate(\n",
    "    lr_pipeline,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "for key, value in lr_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for baseline\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_confusion_matrix(y_test.values, y_pred_lr, title=\"Logistic Regression - Confusion Matrix\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report - Logistic Regression:\")\n",
    "display(get_classification_report_df(y_test.values, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### Interpretation: Baseline Model (Logistic Regression)\n",
    "\n",
    "- **AUC-PR**: `{lr_metrics.get('auc_pr', 0):.4f}` (primary metric for imbalanced data)\n",
    "- **F1-Score**: `{lr_metrics.get('f1', 0):.4f}`\n",
    "- **Precision**: `{lr_metrics.get('precision', 0):.4f}` | **Recall**: `{lr_metrics.get('recall', 0):.4f}`\n",
    "- **Confusion Matrix**: TP={lr_metrics.get('tp', 0):,}, FP={lr_metrics.get('fp', 0):,}, FN={lr_metrics.get('fn', 0):,}, TN={lr_metrics.get('tn', 0):,}\n",
    "\n",
    "**Assessment**\n",
    "- Logistic Regression provides an interpretable baseline with coefficients we can analyze.\n",
    "- The model benefits from SMOTE to handle class imbalance during training.\n",
    "- We'll compare this with an ensemble model to see if added complexity improves performance.\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest pipeline with SMOTE\n",
    "rf_pipeline = build_fraud_pipeline(\n",
    "    model_type=\"random_forest\",\n",
    "    use_smote=True,\n",
    "    numeric_features=NUMERIC_FEATURES,\n",
    "    categorical_features=CATEGORICAL_FEATURES,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "print(\"Random Forest Pipeline:\")\n",
    "print(rf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model, rf_metrics, rf_threshold = train_and_evaluate(\n",
    "    rf_pipeline,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "for key, value in rf_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for ensemble\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "plot_confusion_matrix(y_test.values, y_pred_rf, title=\"Random Forest - Confusion Matrix\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report - Random Forest:\")\n",
    "display(get_classification_report_df(y_test.values, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### Interpretation: Ensemble Model (Random Forest)\n",
    "\n",
    "- **AUC-PR**: `{rf_metrics.get('auc_pr', 0):.4f}`\n",
    "- **F1-Score**: `{rf_metrics.get('f1', 0):.4f}`\n",
    "- **Precision**: `{rf_metrics.get('precision', 0):.4f}` | **Recall**: `{rf_metrics.get('recall', 0):.4f}`\n",
    "- **Confusion Matrix**: TP={rf_metrics.get('tp', 0):,}, FP={rf_metrics.get('fp', 0):,}, FN={rf_metrics.get('fn', 0):,}, TN={rf_metrics.get('tn', 0):,}\n",
    "\n",
    "**Assessment**\n",
    "- Random Forest can capture non-linear relationships and feature interactions.\n",
    "- The ensemble approach typically improves over linear models for complex fraud patterns.\n",
    "- We can extract feature importance for interpretability (see next section).\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = {\n",
    "    \"Logistic Regression + SMOTE\": lr_metrics,\n",
    "    \"Random Forest + SMOTE\": rf_metrics,\n",
    "}\n",
    "\n",
    "comparison_df = compare_models(results)\n",
    "print(\"Model Comparison (sorted by AUC-PR):\")\n",
    "display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_test.values, y_proba_lr, model_name=\"Logistic Regression\", ax=ax)\n",
    "plot_precision_recall_curve(y_test.values, y_proba_rf, model_name=\"Random Forest\", ax=ax)\n",
    "\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_title(\"Precision-Recall Curves - Model Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_confusion_matrix(y_test.values, y_pred_lr, title=\"Logistic Regression\", ax=axes[0])\n",
    "plot_confusion_matrix(y_test.values, y_pred_rf, title=\"Random Forest\", ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_auc_pr = comparison_df.iloc[0]['auc_pr']\n",
    "best_f1 = comparison_df.iloc[0]['f1']\n",
    "\n",
    "lr_auc = lr_metrics.get('auc_pr', 0)\n",
    "rf_auc = rf_metrics.get('auc_pr', 0)\n",
    "improvement = ((rf_auc - lr_auc) / lr_auc * 100) if lr_auc > 0 else 0\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### Interpretation: Model Comparison\n",
    "\n",
    "| Model | AUC-PR | F1 | Precision | Recall |\n",
    "|-------|--------|----|-----------|---------|\n",
    "| Logistic Regression | {lr_metrics.get('auc_pr', 0):.4f} | {lr_metrics.get('f1', 0):.4f} | {lr_metrics.get('precision', 0):.4f} | {lr_metrics.get('recall', 0):.4f} |\n",
    "| Random Forest | {rf_metrics.get('auc_pr', 0):.4f} | {rf_metrics.get('f1', 0):.4f} | {rf_metrics.get('precision', 0):.4f} | {rf_metrics.get('recall', 0):.4f} |\n",
    "\n",
    "**Best Model**: `{best_model_name}` with AUC-PR = `{best_auc_pr:.4f}`\n",
    "\n",
    "**Key Observations**\n",
    "- The ensemble model shows {'improvement' if rf_auc > lr_auc else 'comparable performance'} over the baseline ({improvement:+.1f}% in AUC-PR).\n",
    "- Both models were trained with SMOTE to handle class imbalance.\n",
    "- The PR curves show the precision-recall trade-off at different thresholds.\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest\n",
    "# Get feature names after preprocessing\n",
    "preprocessor = rf_model.named_steps['preprocessor']\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_feature_names = list(cat_encoder.get_feature_names_out(CATEGORICAL_FEATURES))\n",
    "all_feature_names = NUMERIC_FEATURES + cat_feature_names\n",
    "\n",
    "# Get importances\n",
    "rf_classifier = rf_model.named_steps['classifier']\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Features by Importance:\")\n",
    "display(importance_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 15 features\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(top_features['feature'], top_features['importance'], color='#3498db')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title(f'Top {top_n} Feature Importances (Random Forest)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "top5 = importance_df.head(5)['feature'].tolist()\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### Interpretation: Feature Importance\n",
    "\n",
    "**Top 5 most important features**:\n",
    "1. `{top5[0]}`\n",
    "2. `{top5[1]}`\n",
    "3. `{top5[2]}`\n",
    "4. `{top5[3]}`\n",
    "5. `{top5[4]}`\n",
    "\n",
    "**Insights**\n",
    "- Engineered features (time-based, velocity) often rank among the most important.\n",
    "- This validates our Task 1 feature engineering effort.\n",
    "- We'll explore these relationships further with SHAP in Task 3.\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified 5-fold CV for more reliable estimates\n",
    "print(\"Running 5-fold Stratified Cross-Validation for Random Forest...\")\n",
    "print(\"(This may take a few minutes)\")\n",
    "\n",
    "cv_results = cross_validate_model(\n",
    "    rf_pipeline,\n",
    "    X, y,\n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results (5 folds):\")\n",
    "print(f\"  AUC-PR: {cv_results['mean_metrics']['auc_pr']:.4f} ± {cv_results['std_metrics']['auc_pr']:.4f}\")\n",
    "print(f\"  F1:     {cv_results['mean_metrics']['f1']:.4f} ± {cv_results['std_metrics']['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {cv_results['mean_metrics']['roc_auc']:.4f} ± {cv_results['std_metrics']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-fold results\n",
    "cv_df = pd.DataFrame(cv_results['fold_metrics'])\n",
    "display(cv_df[['fold', 'auc_pr', 'f1', 'precision', 'recall', 'roc_auc']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (Random Forest)\n",
    "models_dir = project_root / \"models\"\n",
    "\n",
    "# Save Random Forest (typically best)\n",
    "rf_path = save_model(rf_model, \"fraud_random_forest\", rf_metrics, models_dir)\n",
    "print(f\"Random Forest saved to: {rf_path}\")\n",
    "\n",
    "# Also save Logistic Regression (for comparison / interpretability)\n",
    "lr_path = save_model(lr_model, \"fraud_logistic_regression\", lr_metrics, models_dir)\n",
    "print(f\"Logistic Regression saved to: {lr_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary: Task 2 (E-commerce) Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_metrics = results[best_model_name]\n",
    "\n",
    "cv_auc_mean = cv_results['mean_metrics']['auc_pr']\n",
    "cv_auc_std = cv_results['std_metrics']['auc_pr']\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Summary: Task 2 - E-commerce Fraud Model\n",
    "\n",
    "### Models Trained\n",
    "1. **Logistic Regression + SMOTE** (baseline, interpretable)\n",
    "2. **Random Forest + SMOTE** (ensemble, captures non-linearity)\n",
    "\n",
    "### Best Model: `{best_model_name}`\n",
    "- **Hold-out Test AUC-PR**: `{best_metrics.get('auc_pr', 0):.4f}`\n",
    "- **Hold-out Test F1**: `{best_metrics.get('f1', 0):.4f}`\n",
    "- **5-Fold CV AUC-PR**: `{cv_auc_mean:.4f} ± {cv_auc_std:.4f}`\n",
    "\n",
    "### Model Selection Justification\n",
    "- The ensemble model (Random Forest) was selected based on higher AUC-PR.\n",
    "- Cross-validation confirms stable performance across folds.\n",
    "- Feature importance analysis shows that engineered features contribute meaningfully.\n",
    "\n",
    "### Files Saved\n",
    "- `models/fraud_random_forest.joblib`: Best model\n",
    "- `models/fraud_logistic_regression.joblib`: Baseline model\n",
    "\n",
    "### Next Steps\n",
    "- Run `05_modeling_creditcard.ipynb` for the credit card dataset\n",
    "- Proceed to Task 3 for SHAP explainability analysis\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
